{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -U google-generativeai"
      ],
      "metadata": {
        "id": "O3CJ3h-r9vSw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure with the API key directly\n",
        "genai.configure(api_key='YOUR API KEY')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mgmaTYFGjAj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini model\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# Initialize chat history\n",
        "chat_history = []\n",
        "\n",
        "# Function to get the model's response, focused on interview questions only\n",
        "def get_model_response(previous_answer, chat_history):\n",
        "    # Combine the previous answer with the chat history\n",
        "    prompt = f\"You are conducting a job interview. Based on the candidate's previous answer: '{previous_answer}' and the following chat history: {chat_history}, ask the next interview question. Only provide the question without any additional explanation.\"\n",
        "\n",
        "    # Generate the response using the model\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    # Return only the generated question (without explanations)\n",
        "    return response.candidates[0].content.parts[0].text.strip()\n",
        "\n",
        "# Initial question\n",
        "print(\"Gemini: Tell me about yourself?\")\n",
        "user_answer = input(\"You: \")\n",
        "\n",
        "# Store the initial question and answer in chat history\n",
        "chat_history.append({\"question\": \"Tell me about yourself?\", \"answer\": user_answer})\n",
        "\n",
        "\n",
        "# Main interview loop\n",
        "while True:\n",
        "\n",
        "    # Get the model's response based on the last answer, focused on interview-related questions\n",
        "    model_response = get_model_response(user_answer, chat_history)\n",
        "\n",
        "    # Display the model's response (next interview question) to the user\n",
        "    print(f\"Gemini: {model_response}\")\n",
        "\n",
        "    # Get the user's response and store it\n",
        "    user_answer = input(\"You: \")\n",
        "    chat_history.append({\"question\": model_response, \"answer\": user_answer})\n",
        "\n",
        "\n",
        "    if len(chat_history)>10:\n",
        "        chat_history.pop(0)\n",
        "\n",
        "    # Exit condition (modify as needed)\n",
        "    if user_answer.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "Xmt7AaUkoyZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_vI5PMarAJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}